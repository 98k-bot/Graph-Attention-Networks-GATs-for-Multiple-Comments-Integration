# -*- coding: utf-8 -*-
"""snsum.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tz4IVprPX0oDKrgB740CcCTiG8TOw3QF
"""

from google.colab import drive
drive.mount('/content/drive')

!ls "/content/drive/My Drive"

#%%capture
!unzip drive/My\ Drive/cnn_stories_tokenized.zip

from os import listdir
import string
import numpy as np

# load doc into memory
def load_doc(filename):
	# open the file as read only
	file = open(filename, encoding='utf-8')
	# read all text
	text = file.read()
	# close the file
	file.close()
	return text

load_doc("/content/drive/My Drive/Data-Amazon/human_source/1.txt")
import nltk.data
nltk.download('punkt')
tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')
fp = open("/content/drive/My Drive/Data-Amazon/human_source/1.txt")
data = fp.read()
Sentences = []
Sentences.append(tokenizer.tokenize(data))
#print ("\n".join(tokenizer.tokenize(data)))
print(Sentences[0])

# load all stories in a directory
def load_stories(directory):
	for name in listdir(directory):
		filename = directory + '/' + name
		# load document
		doc = load_doc(filename)

# split a document into news story and highlights
def split_story(doc):
	# find first highlight
	index = doc.find('@highlight')
	# split into story and highlights
	story, highlights = doc[:index], doc[index:].split('@highlight')
	# strip extra white space around each highlight
	highlights = [h.strip() for h in highlights if len(h) > 0]
	return story, highlights

# load all stories in a directory
def load_stories(directory):
	all_stories = list()
	only_sentences = list()
	for name in listdir(directory):
		filename = directory + '/' + name
		# load document
		doc = load_doc(filename)
		# split into story and highlights
		story, highlights = split_story(doc)
		# store
		all_stories.append({'story':story, 'highlights':highlights})					#uncomment this line to return all_stories
		only_sentences.append(story)
	return only_sentences,all_stories										#also returns all_stories but removed to save ram

directory = '/content/cnn_stories_tokenized'
only_sentences, stories = load_stories(directory)
print('Loaded Stories %d' % len(only_sentences))



only_sentences[0]

# clean a list of lines
def clean_lines(lines):
	cleaned = list()
	# prepare a translation table to remove punctuation
	table = str.maketrans('', '', string.punctuation)
	#print(len(lines))
	for line in lines:
		# strip source cnn office if it exists
		index = line.find('(CNN) -- ')
		if index > -1:
			line = line[index+len('(CNN)'):]
		# tokenize on white space
		line = line.split()
		# convert to lower case
		line = [word.lower() for word in line]
		# remove punctuation from each token
		line = [w.translate(table) for w in line]
		# remove tokens with numbers in them
		line = [word for word in line if word.isalpha()]
		# store as string
		cleaned.append(' '.join(line))
	# remove empty strings
	cleaned = [c for c in cleaned if len(c) > 0]
	return cleaned

# clean stories
for example in stories:
	example['story'] = clean_lines(example['story'].split('\n'))
	example['highlights'] = clean_lines(example['highlights'])
 
# clean only_sentences
count=0
cleaned_sentences = []
len_each_doc=[]
progress=10
for example in only_sentences:
	count+=1
	percent = (count/92579)*100
	percent = int(percent)
	if (percent) == progress:
		print(percent)
		progress+=10
	cleaned_sentences.extend(clean_lines(example.split('\n')))
len(cleaned_sentences)
#len(stories[1]['story'])

only_sentences[0:15]

def convert(lst): 
    a = []
    for i in range(15):
      a.append(lst[i].split())
    return (a) 
print(convert(only_sentences))
word_embedding = convert(only_sentences)

print(convert(Sentences[0]))
word_embedding = convert(Sentences[0])

import os
if 'COLAB_TPU_ADDR' not in os.environ:
  print('ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!')
else:
  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']
  print ('TPU address is', tpu_address)



#####  faster implementation  #####


import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() 
import tensorflow_hub as hub

# Create graph and finalize (finalizing optional but recommended).
g = tf.Graph()
with g.as_default():
  # We will be feeding 1D tensors of text into the graph.
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/2")
  embedded_wordtext = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Create session and initialize.
session = tf.Session(graph=g)
session.run(init_op)
word_result = []
lword_result = []
for i in range(15):
  word_result.append(session.run(embedded_wordtext, feed_dict={text_input: word_embedding[i]}))
for i in range(15):
  lword_result.append(np.array_str(word_result[i]))
print(lword_result[14])

#####  faster implementation  #####


import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() 
import tensorflow_hub as hub

# Create graph and finalize (finalizing optional but recommended).
g = tf.Graph()
with g.as_default():
  # We will be feeding 1D tensors of text into the graph.
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/2")
  embedded_text = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Create session and initialize.
session = tf.Session(graph=g)
session.run(init_op)

result = session.run(embedded_text, feed_dict={text_input: lword_result})
print(result)
str_result = []
for i in range(15):
  str_result.append(np.array_str(result[i]))

import tensorflow as tf
import tensorflow.compat.v1 as tf
tf.disable_v2_behavior() 
import tensorflow_hub as hub

# Create graph and finalize (finalizing optional but recommended).
g = tf.Graph()
with g.as_default():
  # We will be feeding 1D tensors of text into the graph.
  text_input = tf.placeholder(dtype=tf.string, shape=[None])
  embed = hub.Module("https://tfhub.dev/google/universal-sentence-encoder/2")
  embedded_text = embed(text_input)
  init_op = tf.group([tf.global_variables_initializer(), tf.tables_initializer()])
g.finalize()

# Create session and initialize.
session = tf.Session(graph=g)
session.run(init_op)

sec_decode = session.run(embedded_text, feed_dict={text_input: str_result})
ed_result = []
for i in range(15):
  ed_result.append(result[i])
for i in range(15):
  ed_result.append(sec_decode[i])
ed_result = np.asarray(ed_result)
print(ed_result)

from sklearn.metrics.pairwise import cosine_similarity
cos=cosine_similarity(ed_result)
cos

#adjacency matrix
size=ed_result.shape[0]
#size = 30
adj_matrix=np.zeros((size,size))
ii=0
jj=0
for i in range(size):
  for j in range(size):
    if cos[i][j]>0.9:   
      adj_matrix[i][j] = 1
    else:
      adj_matrix[i][j] = 0
adj_matrix

np.matmul(adj_matrix,ed_result)

D = np.array(np.sum(adj_matrix, axis=0))
D = np.matrix(np.diag(D))

seed=22321
np.random.seed(seed)
W_1 = tf.Variable(tf.random_normal([512, size], name='W_1'))
W_2 = tf.Variable(tf.random_normal([W_1.shape[1], size], name='W_2')) 
# W_2 = np.random.normal(
#     loc=0, size=(W_1.shape[1], size))

W_1=tf.cast(W_1, tf.float64)
W_2=tf.cast(W_2, tf.float64)

def gcn_layer(A_hat, D_hat, X, W):
  Di = np.linalg.inv(D_hat)
  print(Di.shape)
  print(A_hat.shape)
  #print(X.shape)
  print(W.shape)
  X=tf.cast(X, tf.float64)
  A_hat=tf.cast(A_hat, tf.float64)
  # W=tf.cast(W, tf.float64)
  aaa=tf.linalg.matmul(Di,A_hat)
  bbb=tf.linalg.matmul(X,W)
  ccc=tf.linalg.matmul(aaa,bbb)
  #above 3 calcs are equal to Di * A_hat * X * W
  return tf.nn.relu(ccc)
    
#adj_matrix, D, message_embeddings, W

H_1 = gcn_layer(adj_matrix, D, ed_result, W_1)
H_2 = gcn_layer(adj_matrix, D, H_1, W_2)

output = H_2

ed_result=tf.cast(ed_result, tf.float64)
Di = np.linalg.inv(D)
inp=tf.linalg.matmul(tf.linalg.matmul(Di,adj_matrix), tf.linalg.matmul(ed_result,W_1) )
#inp=Di * adj_matrix * result * W_1
inp=tf.convert_to_tensor(inp) 
# with tf.Session() as sess: 
#     print('Input type:', inp) 
#     print('Input:', sess.run(inp)) 
#     print('Return type:', H_2) 
#     print('Output:', sess.run(H_2)) 
# print(H_2==0)

#salience estimation
np.random.seed(seed)
salienceW = tf.Variable(tf.random_normal([size, size], name='salienceW'))
#salienceW = np.random.normal(loc=0, scale=1, size=(size,size))
salienceW=tf.cast(salienceW, tf.float64)
tanned=tf.nn.sigmoid(tf.linalg.matmul(H_2 , salienceW))
# with tf.Session() as sess:
#   out_tan=sess.run(tanned)

#print("tan=",out_tan.shape)

v = tf.Variable(tf.random_normal([size,1], name='v'))
#v = np.random.normal(loc=0, scale=1, size=(size,1))

v=tf.cast(v, tf.float64)

fs=tf.linalg.matmul(tanned, v)
# with tf.Session() as sess:
#   fs=sess.run(tf.linalg.matmul(tanned, v))
# fs.shape

sum_salience=tf.reduce_sum(fs, 0)
# with tf.Session() as sess:
#   sum_salience=sess.run(tf.reduce_sum(fs, 0))
  #salience=sess.run(fs/sum_salience)

salience=fs/sum_salience
# #print (fs)
# print (salience)
# idx=salience.argmax()
# print(idx)
# print(cleaned_sentences[idx])
# stories[0]['highlights']
print(salience)

# calculation of rouge

a=stories[0]['highlights']
overlap=[]
for i in range(len(cleaned_sentences[0:30])):
  overlap.append(0)
  for j in cleaned_sentences[i].split():
    result=a[0].find(j)
    if result!=-1:
     overlap[i]+=1
     #print(j)
rouge1=[]
for d in overlap:
  t=d/len(a[0])
  rouge1.append(t)
rouge1[0:100]

smR1=tf.nn.softmax(rouge1)
fin_smR1=smR1
# with tf.Session() as sess: 
#   fin_smR1=sess.run(smR1)
fin_smR1

#Calculating cross entropy
fin_smR1=tf.cast(fin_smR1, tf.float64)
fin_smR1 = tf.reshape(fin_smR1,[1,30])




#y_clipped = tf.clip_by_value(y_, 1e-10, 0.9999999)
cross_entropy_ob = -tf.linalg.matmul( fin_smR1, (salience))

#optimmizer

optimiser = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(cross_entropy_ob)

init_op = tf.initialize_all_variables()

sess = tf.Session()
sess.run(init_op)

inp_np=inp.eval(session=sess)



fin_smR1_np=fin_smR1.eval(session=sess)

salience_np=salience.eval(session=sess)


dataX = tf.placeholder(tf.float64, [None, 30])
# now declare the output data placeholder - 10 digits
labelY = tf.placeholder(tf.float64, [None, 30])

init_op = tf.global_variables_initializer()
total_batch=8

correct_prediction = tf.equal(tf.argmax(fin_smR1, 1), tf.argmax(salience_np, 1))
accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))

# start the session
with tf.Session() as sess:
   # initialise the variables
   sess.run(init_op)
   #total_batch = int(len(mnist.train.labels) / 100)
   for epoch in range(10000):
        avg_cost = 0
        for i in range(total_batch):
            batch_x = inp_np
            batch_y = fin_smR1_np
            _, c = sess.run([optimiser, cross_entropy_ob], feed_dict={dataX: batch_x, labelY: batch_y})
            avg_cost += c / total_batch
        #print("Epoch:", (epoch + 1), "cost =", "{:.3f}".format(avg_cost))
        print("Epoch:", (epoch + 1))
   #print(sess.run(accuracy, feed_dict={x: batch_x, y: batch_y}))

max_att = []
print(fin_smR1_np)
for j in range(14):
  attention = []
  sum_dattention = 0
  for i in range(14):
    sum_dattention += abs(fin_smR1_np[0][i+1+j]-fin_smR1_np[0][i+j])
  for i in range(14):
    attention.append(abs(fin_smR1_np[0][i+1+j]-fin_smR1_np[0][i+j])/sum_dattention)
  max_att.append(attention)
print(np.asarray(max_att).shape)

import matplotlib.pyplot as plt
import numpy as np
import sklearn
from numpy.linalg import norm
import matplotlib.pylab as pylab
params = {'legend.fontsize': 'x-large',
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize':'x-large',
         'ytick.labelsize':'x-large'}
pylab.rcParams.update(params)
nodeDegree = []
salience_nplist = []
a = np.random.random((16, 16))

plt.imshow(np.asarray(max_att), cmap='Blues', interpolation='nearest')
plt.colorbar()
plt.title('Attention')

import matplotlib.pyplot as plt
import numpy as np
import sklearn
from numpy.linalg import norm
import matplotlib.pylab as pylab
params = {'legend.fontsize': 'x-large',
         'axes.labelsize': 'x-large',
         'axes.titlesize':'x-large',
         'xtick.labelsize':'x-large',
         'ytick.labelsize':'x-large'}
pylab.rcParams.update(params)
nodeDegree = []
salience_nplist = []
a = np.random.random((16, 16))

plt.imshow(cos, cmap='Blues', interpolation='nearest')
plt.colorbar()
plt.title('Sentence Relation Graph')
print(cos)
#plt.savefig("/content/drive/My Drive/SRG_A")
plt.show()
cos.shape
#print(cos.shape)
for i in range(15):
  nodeDegree.append(sum(cos[i]))
#print(nodeDegree)
tf.Print(salience,[salience])
#print(salience_np)
for i in range(15):
  salience_nplist.append(abs(salience_np[i][0]))
print(salience_nplist)
plt.scatter(salience_nplist,nodeDegree)
cosSI = np.inner(salience_nplist, nodeDegree)/(norm(salience_nplist)*norm(nodeDegree))
print(cosSI)
plt.ylabel('Node degrees')
plt.xlabel('Salience scores')
plt.title('ADG, Correlation Coefficient: 0.54')
#plt.savefig("/content/drive/My Drive/ADG_B")
plt.show()

import networkx as nx
import numpy as np

G = nx.from_numpy_matrix(np.array(adj_matrix)) 
pos = nx.spring_layout(G,k=0.8,iterations=1)
nx.draw(G, with_labels=True,pos=pos)


adj_matrix.sum()